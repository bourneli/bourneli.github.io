---
layout: post
title:  World Model-简化梦境结构，大幅提升训练效率。
categories: [LLM,World Model] 
---

当代社会中，如果不会理财，每天作牛作马获得的血汗钱就会被通货膨胀这张无形的大手偷偷拿走。这是学习理财最朴素的动机。理财到现在，接触过很多产品，从最开始的定期存储，到货币基金，到各种各样基金，国债，黄金，指数基金，个股等都有过接触。因为毕竟咱们还有正经的工作，理财主打一个“业余”，不能花费太多经历在理财上，利用有限的精力，拿到稳健且正向的收益最为重要。

目前，本人已经有15年业余理财经验，经历过一个小牛熊转换，整体收益正向且稳健，尤其是最近5年。在这个过程中，本人的心智得到了很好的历练，并且会将理财中的一些底层逻辑是可以迁移到工作中来的。比如，工作中做任何事情，都会思考其ROI，以及这个事情和其他事情的关系，来综合评判是否值得做，怎么做，如何投入。由于理财收益有稳定且可观的回报，使得自己在生活和工作中底气更足，可以帮助自己看清工作中压力的本质，以及压力的源头，从而进行决策和应对。

理财是个好东西，希望每个人都能够掌握，为自己的生活添加坚实的后盾。具体如何理财，如何实现正向的收益，互联网上的内容非常多，而且现在还有LLM可以帮助检索出更多内容，只要你有动力学习理财，总可以找到途径。但是，切记不能贪心，理财是管理和驾驭财富的能力，而不是赌博技巧。

转到世界模型的[Dreamer V2](https://arxiv.org/abs/2010.02193)上，该模型在Dreamer (2020) 的基础上，通过一系列关键修改实现了性能突破：

- **离散潜在变量**：将世界模型中的潜在状态从**高斯分布**（连续）改为**分类分布**（离散）。这是最主要的改进之一，实验证明能显著提升性能。在**相同计算预算**（单GPU，约10天）和**环境步数**（200M帧）下，DreamerV2的最终性能超越了Rainbow和IQN等顶尖无模型算法，证明了基于模型方法的竞争力。
- **KL平衡**：在世界模型训练中，对先验分布和后验分布的KL散度损失采用**不同的学习率**（先验0.8，后验0.2），以鼓励学习更准确的先验，而非单纯增加后验熵。
- **策略梯度混合**：结合了**Reinforce梯度**（无偏但高方差）和**直通梯度**（有偏但低方差）来训练行动者网络。在Atari任务中，主要依赖Reinforce梯度。

**消融实验结论**也显示离散潜在变量和KL平衡对性能提升贡献最大。

总而言之，[Dreamer (2020)](https://arxiv.org/pdf/1912.01603)是**开创者**，它确立了“在学得的世界模型的潜在空间中进行想象并反向传播学习策略”这一范式，在连续控制任务上证明了其高效性。**DreamerV2 (2021)** 是**突破者**，它将这一范式推向了新的高度。通过引入**离散潜在变量**和**KL平衡**等关键技术，解决了在更复杂、更具挑战性的Atari游戏上世界模型的精度和泛化问题，最终实现了**纯基于模型的智能体首次在Atari上达到人类水平**，标志着基于模型的强化学习在经典基准上具备了与顶尖无模型方法正面竞争的实力。

































