---
layout: post
title:  World Model的重大改进-PlaNet
categories: [LLM,World Model]
---

最近国内海外都一些报告表示这次2022年Chatgpt+StableDiffusion引领的AI热潮的存在明显的科技泡沫。其主要论据是卖铲子的nVidia挣的[盆满钵满](https://www.captide.ai/insights/nvidia-q3-2025)，其财报显示Q3营收350亿美元；但是用这些铲子挖矿的大厂目前都没有明显盈利，其中Open AI在2025年Q3亏损达[120亿美元](https://www.wsj.com/livecoverage/stock-market-today-dow-sp-500-nasdaq-10-31-2025/card/openai-made-a-12-billion-loss-last-quarter-microsoft-results-indicate-e71BLjJA0e2XBthQZA5X)。这组数据看起来比较反直觉。

作为AI的从业者，我们的工作模式也不可避免的在这个浪潮中被改变。由于GPU的昂贵价格，以及训练大模型需要的海量数据。一些中小型团队根本无法支付这种成本，现在大多数工作只能是在模型应用层面，去解决应用场景最后一公里的事情。偶尔也会微调模型，针对语言模型的SFG，目前成本相对较低，每次模型迭代需要的费用大概在**几百美元**。如果需要SFG视觉模型，对显卡的需求要求更高，每次微调的成本可达到几千美元。SFG的成本都这么高，更别谈去根据业务的需求，优化模型的其他解构。回想10年前，深度学习开始流行的时候，GPU没有这么昂贵，每次训练也不需要这么多数据，AI从业人员可以相对容易的根据业务需求修改模型，以得到更好的结果，为企业带来价值。讲道理，技术的进步应该让更多人受益，应该允许更多人参与技术的迭代和演进。但事与愿违，整个学术界和工业界都因为GPU的匮乏而无法开展工作，这类工作成逐渐成为了一些“GPU门阀”的特权。

今天重温了一篇2019年的世界模型的经典文章，[规划网络PlaNet](https://arxiv.org/abs/1811.04551)。这篇继承了2018年的论文[World Model](https://arxiv.org/abs/1803.10122)的核心思想，但是在模型细节上进行了改进，使得智能体可以在此方法上解决更复杂的任务。PlanNet的主要思路仍然是训练一个世界模型来模拟环境，但是这个世界模型不是直接根据随机的数据来学习的，而是和智能体一起迭代学习。即先随机初始的收集环境数据，然后用这个数据训练智能体，再用这个智能体和真实的环境交互收集新的数据，并将这些数据加入到之前的数据集中继续在训练环境，如此往复，世界模型和智能体交替迭代演进。这样带来的好处是极大的减少智能体和真实环境交互的次数，非常适合交互成本较高的场景。而且智能体也是“梦境”中进行操作，而不是在视觉空间，这样也极大的提升的计算效率。同时，世界模型的架构模为RSSM（=GRU+随机性路径），比World Model中的VAE+RNN要更加先进。那个时候，学术界和工业界还可以在模型和范式层面做很多尝试，每年总是有经典的论文问世。

如果当前AI泡沫的破灭，可以使AI的演进权从“GPU门阀”下放到“平明百姓”。也许破灭是个不错的结果。只有浪潮褪去了，才知道谁在裸泳。



















