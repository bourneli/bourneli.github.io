---
layout: post
title:  World Model-Dreamer V3工程化的卓越整合与优化
categories: [LLM,World Model] 
---

追求确定性，有错吗？这是我最近一直在思考的问题。做任何事情都有风险，这个是客观存在的现象，只是有些风险大，有些风险小。沃伦巴菲特一辈子都在寻找确定性资产，比如他买的可口可乐，因为他相信人们喜欢喝糖水的爱好，过去如此，现在如此，将来仍然如此，所以这笔投资是确定性非常大的。在强化学习中，有一个策略叫做EE（Exploit & Explore），Exploit就是基于确定性的方向，不断获得奖励，而Explore就是在未知的领域探索，以期望得到更多收益。因此，追求确定性不是目的，追求确定性后面的收益，那些可以用真金白银衡量的东西，才是真正的目的。所以，回到开头的那个问题，应该是有答案了，即追求的是收益最大化，确定性或不确定性都是达到目标的不同轨迹，而不是追求的目标。

现实生活中，存在富人家的孩子和穷人家的孩子。富人家的孩子，天生就自带资源，人生的容错率远远高于穷人家的孩子。就好比打游戏，富人家的孩子在新手村就是一套神装，而穷人家的孩子必须按照新手村的指引才能积累到足够的装备离开新手村。所以，富人家的孩子可以尝试各种不同寻常的人身轨迹，体验不同的新奇事物，并且让这些经历成为他日后成就事业的基础。而穷人家的小孩，大多通过通过小镇“做题家”的轨迹，离开新手村。这个故事告诉我们，有钱就可以任性的试错，不计成本，而没钱就只能老老实实沿着成功率最大的路径前进。但是有钱或者没钱也是相对的。比如你在一个大公司工作，可能是“有钱人”的孩子，有很多资源可以拿来试错。但是，你也许是在这个大公司的“清水衙门”，没有多少收益，又或者你所在的组织面临较大的盈利指标，那这个时候你也许就没有太多资源去试错。

这里，存在一个四象限，横轴是富裕or贫穷，纵轴是探索or利用。大部分情况会落入到富裕-探索或贫穷-利用这两个象限里面。也许还有一部分人在富裕-利用这个象限里面，但是应该不多，毕竟钱多了就想尝试一些新鲜事物，除非挣钱本身是你的爱好，比如沃伦巴菲特。但是如果落入到贫穷-探索的这个象限里面，就要警惕了，虽然穷者思变，但大概率是九死一生。如果你确实喜欢这个探索的方向，那么你应该找个靠谱的富人作为护盾，这样才能保证生存的情况下继续探索，否则就是浪费时间和心力。如果找不到富人，那就找个靠谱的方向，极限榨干里面的价值，使得自己成为富人，然后再去探索新的方向。比如DeepSeek就是一个典型的例子，其母公司幻方通过量化交易挣到了足够多的资本，创始人才有本钱去做自己喜欢的科技创新研究项目DeepSeek。

回到论文[Dreamer V3](https://arxiv.org/pdf/2301.04104)，这是一篇利用大量优化技巧和工程手段，将Dream框架打造成一个非常通用的世界模型，一套超参数可以适应150+的不同任务，并且都达到了当时的Sota。具体使用了如下的一些优化技巧，

1. **鲁棒预测技术**：
    - **Symlog变换**：对向量观测进行对称对数变换（`symlog`），以处理可能具有未知数量级和长尾分布的输入/输出信号，防止大值梯度爆炸和小值梯度消失。
    - **Synexp Twohot损失**：为奖励预测器和评论家网络设计了一种新的损失函数，结合了`symexp`（对称指数）变换和`two-hot`编码，以鲁棒地预测可能具有任意尺度的标量值（如奖励）。

2. **世界模型训练的稳定性增强**：
    - **KL平衡与自由比特**：在变分世界模型的KL散度损失中，对先验交叉熵和后验熵使用**不同的权重**（先验0.8，后验0.2），以鼓励学习更准确的动态先验，而非单纯压缩后验。结合“自由比特”技术，防止表征损失过度正则化。
    - **1%均匀混合**：将编码器和动态预测器输出的分类分布参数化为**1%均匀分布与99%神经网络输出的混合**，防止分布变得确定（熵为零），从而确保KL损失行为良好，避免训练中出现尖峰。

3. **演员-评论家学习的鲁棒性改进**：

   - **回报归一化**：为了在**不同奖励频率和稀疏度**的任务中使用**固定的熵正则化系数**，对想象轨迹的回报进行归一化，将其大致缩放到`[0, 1]`区间。为了避免在稀疏奖励下放大近似误差，只对超过阈值`L=1`的大回报进行缩放。
   - **评论家回放损失**：除了在想象轨迹上训练评论家，**额外在从回放缓冲区采样的真实轨迹上训练评论家**，使用想象起点的价值作为“同策略”价值标注，以改善在奖励难以预测的环境中的价值估计。

4. **网络架构与优化器改进**：
- **块对角GRU**：使用具有**块对角循环权重**的GRU作为序列模型，允许使用大量记忆单元而不会导致参数量和计算量的平方增长。
  
- **自适应梯度裁剪**：采用AGC，根据权重矩阵的范数来裁剪每层张量的梯度，将裁剪阈值与损失尺度解耦，提高了稳定性。
  
- **LaProp优化器**：使用LaProp（RMSProp后加动量）代替Adam，避免了Adam中因对原始梯度同时计算动量和归一化器而可能引发的不稳定问题。

DreamerV3的创新本质是**“工程上的卓越整合与优化”**，此方法就是将现有资源Exploit到了极致的典型例子。它没有颠覆Dreamer的核心范式（学习世界模型 + 在潜在空间中想象学习），而是通过一系列精巧的设计，解决了将这一范式**大规模、稳定地应用于极其多样的现实世界任务时所遇到的工程挑战**。这些挑战包括：不同环境的信号尺度差异巨大、奖励稀疏性不同、训练动态不稳定等。因此，DreamerV3的主要贡献在于**提供了一套经过充分验证的、可复现的“配方”**，使得基于世界模型的强化学习从一个需要精心调参的“研究原型”，转变为一个**强大、稳定、可扩展的通用智能体框架**，为强化学习的实际应用铺平了道路。其论文标题和摘要反复强调的“**a single configuration**”和“**out of the box**”，正是其核心创新价值所在。

至此，世界模型的Dreamer系列完结，来到23年，基于LLM和Diffusion的世界模型逐渐吸引了全世界学者的注意力。后面，会Explore更多世界模型的研究方向。





























